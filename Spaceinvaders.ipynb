{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow gym keras-rl2 gym[atari]\n",
    "%pip install -U gym\n",
    "%pip install -U gym[atari,accept-rom-license]\n",
    "%pip install -U gym[ale,accept-rom-license]\n",
    "%pip install gym[atari]\n",
    "%pip install autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d599c",
   "metadata": {},
   "source": [
    "To be able to render the openAI environment and take random steps inside the environment to see how it perform the following need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd52a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym       #to be able to render the openAI environment\n",
    "import random    #to take some random steps inside the environment to see how it performs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3f7f5",
   "metadata": {},
   "source": [
    "To generate the environment and see the spaceinvaders game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4288ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Musau\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v4', render_mode='human')   #Allows to generate environment\n",
    "height, width, channels = env.observation_space.shape #taking, height width and channels to shape the image \n",
    "actions = env.action_space.n       #gives number of actions to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee0c40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings() #To know the actions that the spaceship in the game has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "799c0a91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:105.0\n",
      "Episode:2 Score:20.0\n",
      "Episode:3 Score:115.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 3           #playing 5 different games of spaceinvaders\n",
    "for episode in range(1, episodes+1):    #looping through the episodes\n",
    "    #resetting the variables\n",
    "    state = env.reset()\n",
    "    done = False        #flag for if game is done it is stopped\n",
    "    score = 0           #counter\n",
    "    \n",
    "    while not done:\n",
    "        action = random.choice([0,1,2,3,4,5])  #random choice out of the above actions\n",
    "        n_state, reward, done, info = env.step(action)    #grab the following parameters and take the actions applied above and apply them on the env\n",
    "        score+=reward   #append the score to the counter\n",
    "    print(f\"Episode:{episode} Score:{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa750e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757883e",
   "metadata": {},
   "source": [
    "# Creating a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43798fd8",
   "metadata": {},
   "source": [
    "The Deep learning Model will be used side-by-side with the agent to 'learn' how best to operate in the OpenAI environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff54609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caa7d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to define what our deep learning model will look like\n",
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    #to add layers on to our neural network\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3, height, width, channels)))\n",
    "    #stack other convolution layers on top\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    #one with a one by one stride going pixel by pixel\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    \n",
    "    #takes all of the layers and flatten into a single layer in order to parse into the Dense layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Dense layers/fully connected layers meaning each unit in that particular layer is connected to every single unit in the next layer\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))#6 units each one rep an action\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d5f76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17586bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model\n",
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c896cbe",
   "metadata": {},
   "source": [
    "# Building the Keras-RL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e93205",
   "metadata": {},
   "source": [
    "Duelling Networks split value and advantage and they help the model learn when to take action and when not to bother. Not so much a competing but a modified network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e237bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent #the reinforcement agent we will be using\n",
    "from rl.memory import SequentialMemory  #to hold the knowledge buffer for the reinforcement learning agents.\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy #EP to find the best reward outcome, linear to give a little bit of decay in order to close in on an optimal strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a83c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build our Keras-RL agent\n",
    "def build_agent(model, actions):\n",
    "    #define what our search and decay looks like\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    \n",
    "    #using Sequential Memory\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    \n",
    "    #defining the dqn agent\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg',\n",
    "                  nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0164c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4)) #using Adam optimizer to optimize our method with a learning rate of .0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3293bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Musau\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Musau\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  384/10000: episode: 1, duration: 39.594s, episode steps: 384, steps per second:  10, episode reward: 20.000, mean reward:  0.052 [ 0.000, 10.000], mean action: 2.503 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  842/10000: episode: 2, duration: 45.060s, episode steps: 458, steps per second:  10, episode reward: 65.000, mean reward:  0.142 [ 0.000, 20.000], mean action: 2.341 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Musau\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1318/10000: episode: 3, duration: 531.341s, episode steps: 476, steps per second:   1, episode reward: 120.000, mean reward:  0.252 [ 0.000, 30.000], mean action: 2.445 [0.000, 5.000],  loss: 9.649218, mean_q: 3.604913, mean_eps: 0.895690\n",
      " 1619/10000: episode: 4, duration: 462.715s, episode steps: 301, steps per second:   1, episode reward: 55.000, mean reward:  0.183 [ 0.000, 20.000], mean action: 2.349 [0.000, 5.000],  loss: 0.938952, mean_q: 2.848165, mean_eps: 0.867880\n",
      " 2432/10000: episode: 5, duration: 7538.240s, episode steps: 813, steps per second:   0, episode reward: 245.000, mean reward:  0.301 [ 0.000, 30.000], mean action: 2.480 [0.000, 5.000],  loss: 1.007363, mean_q: 3.509114, mean_eps: 0.817750\n",
      " 2920/10000: episode: 6, duration: 794.197s, episode steps: 488, steps per second:   1, episode reward: 105.000, mean reward:  0.215 [ 0.000, 25.000], mean action: 2.547 [0.000, 5.000],  loss: 0.790689, mean_q: 3.794214, mean_eps: 0.759205\n",
      " 3512/10000: episode: 7, duration: 1083.512s, episode steps: 592, steps per second:   1, episode reward: 95.000, mean reward:  0.160 [ 0.000, 30.000], mean action: 2.449 [0.000, 5.000],  loss: 0.722640, mean_q: 3.132163, mean_eps: 0.710605\n",
      " 3889/10000: episode: 8, duration: 756.520s, episode steps: 377, steps per second:   0, episode reward: 50.000, mean reward:  0.133 [ 0.000, 20.000], mean action: 2.645 [0.000, 5.000],  loss: 0.384910, mean_q: 2.590249, mean_eps: 0.667000\n",
      " 4140/10000: episode: 9, duration: 499.334s, episode steps: 251, steps per second:   1, episode reward: 15.000, mean reward:  0.060 [ 0.000, 10.000], mean action: 2.084 [0.000, 5.000],  loss: 0.167306, mean_q: 2.504947, mean_eps: 0.638740\n",
      " 4431/10000: episode: 10, duration: 587.390s, episode steps: 291, steps per second:   0, episode reward: 115.000, mean reward:  0.395 [ 0.000, 30.000], mean action: 2.577 [0.000, 5.000],  loss: 0.730374, mean_q: 2.941101, mean_eps: 0.614350\n",
      " 5266/10000: episode: 11, duration: 1634.208s, episode steps: 835, steps per second:   1, episode reward: 240.000, mean reward:  0.287 [ 0.000, 30.000], mean action: 2.582 [0.000, 5.000],  loss: 0.587543, mean_q: 3.538735, mean_eps: 0.563680\n",
      " 5524/10000: episode: 12, duration: 423.219s, episode steps: 258, steps per second:   1, episode reward: 15.000, mean reward:  0.058 [ 0.000, 10.000], mean action: 2.329 [0.000, 5.000],  loss: 0.175790, mean_q: 3.911180, mean_eps: 0.514495\n",
      " 6086/10000: episode: 13, duration: 934.401s, episode steps: 562, steps per second:   1, episode reward: 130.000, mean reward:  0.231 [ 0.000, 30.000], mean action: 2.238 [0.000, 5.000],  loss: 0.366170, mean_q: 4.041132, mean_eps: 0.477595\n",
      " 6388/10000: episode: 14, duration: 472.884s, episode steps: 302, steps per second:   1, episode reward: 105.000, mean reward:  0.348 [ 0.000, 30.000], mean action: 2.798 [0.000, 5.000],  loss: 0.353624, mean_q: 3.227292, mean_eps: 0.438715\n",
      " 6929/10000: episode: 15, duration: 876.443s, episode steps: 541, steps per second:   1, episode reward: 155.000, mean reward:  0.287 [ 0.000, 30.000], mean action: 2.468 [0.000, 5.000],  loss: 0.563266, mean_q: 3.186924, mean_eps: 0.400780\n",
      " 7481/10000: episode: 16, duration: 891.236s, episode steps: 552, steps per second:   1, episode reward: 195.000, mean reward:  0.353 [ 0.000, 25.000], mean action: 2.513 [0.000, 5.000],  loss: 0.285308, mean_q: 3.215105, mean_eps: 0.351595\n",
      " 7854/10000: episode: 17, duration: 610.381s, episode steps: 373, steps per second:   1, episode reward: 70.000, mean reward:  0.188 [ 0.000, 25.000], mean action: 2.515 [0.000, 5.000],  loss: 0.413989, mean_q: 3.069762, mean_eps: 0.309970\n",
      " 8226/10000: episode: 18, duration: 598.191s, episode steps: 372, steps per second:   1, episode reward: 110.000, mean reward:  0.296 [ 0.000, 30.000], mean action: 2.137 [0.000, 5.000],  loss: 0.248134, mean_q: 2.971307, mean_eps: 0.276445\n",
      " 8989/10000: episode: 19, duration: 1190.276s, episode steps: 763, steps per second:   1, episode reward: 255.000, mean reward:  0.334 [ 0.000, 30.000], mean action: 2.109 [0.000, 5.000],  loss: 0.503309, mean_q: 3.128849, mean_eps: 0.225370\n",
      " 9363/10000: episode: 20, duration: 568.249s, episode steps: 374, steps per second:   1, episode reward: 30.000, mean reward:  0.080 [ 0.000, 10.000], mean action: 2.586 [0.000, 5.000],  loss: 0.222831, mean_q: 3.578794, mean_eps: 0.174205\n",
      " 9659/10000: episode: 21, duration: 450.702s, episode steps: 296, steps per second:   1, episode reward: 65.000, mean reward:  0.220 [ 0.000, 25.000], mean action: 2.713 [0.000, 5.000],  loss: 0.184099, mean_q: 3.189219, mean_eps: 0.144055\n",
      "done, took 21520.093 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x148c28cb220>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613f346",
   "metadata": {},
   "source": [
    "# Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdaa1b6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dqn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSavedWeights/10k-Fast/dqn_weights.h5f\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dqn' is not defined"
     ]
    }
   ],
   "source": [
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weights.h5f')#saving the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cffd2cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Musau\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "dqn=build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))\n",
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a635d52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 3 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Musau\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 180.000, steps: 899\n",
      "Episode 2: reward: 285.000, steps: 1248\n",
      "Episode 3: reward: 185.000, steps: 812\n",
      "216.66666666666666\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=3, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac11ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
